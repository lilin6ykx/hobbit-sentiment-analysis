{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cysumwqQFk8K","executionInfo":{"status":"ok","timestamp":1760004719863,"user_tz":-120,"elapsed":6434,"user":{"displayName":"Lilin Qiu","userId":"05133583203755337087"}},"outputId":"18285c5b-23a4-46db-eeb4-e372d3a36b55"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_1_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_10_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_11_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_12_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_13_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_14_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_15_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_16_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_17_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_18_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_19_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_2_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_3_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_4_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_5_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_6_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_7_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_8_dialogues.csv\n","Filtered file saved: /content/drive/MyDrive/digphil/dialogues_filtered/chapter_9_dialogues.csv\n","✅ All dialogue files processed and saved to 'dialogues_filtered' folder (CSV only).\n"]}],"source":["import os\n","import glob\n","import requests\n","import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","from google.colab import drive\n","try:\n","    drive.flush_and_unmount()\n","except Exception:\n","    pass\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Download NLTK resources\n","nltk.download('punkt', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","\n","# ---------------- Normalization + Tokenization + Cleaning ---------------- #\n","import re, unicodedata\n","\n","# Negations & intensifiers to preserve (important for sentiment)\n","NEGATIONS = {\"not\", \"no\", \"nor\", \"n't\", \"never\", \"none\", \"nobody\", \"nowhere\", \"neither\"}\n","INTENSIFIERS = {\"very\", \"so\", \"too\", \"quite\", \"rather\", \"really\", \"extremely\", \"highly\", \"barely\", \"hardly\"}\n","\n","# auxiliaries where \"aux n't\" -> \"not\" is a useful simplification\n","_AUX_FOR_NT = {\n","    \"do\",\"does\",\"did\",\n","    \"is\",\"are\",\"was\",\"were\",\n","    \"have\",\"has\",\"had\",\n","    \"will\",\"would\",\"can\",\"could\",\"should\",\"shall\",\"may\",\"might\",\"must\"\n","}\n","# NEW: stems produced by tokenization for can't/won't/shan't\n","_AUX_NT_STEMS = {\"ca\", \"wo\", \"sha\"}\n","\n","# keep only alphabetic tokens a–z (drop ALL punctuation and numbers)\n","_ALPHA_RE = re.compile(r\"^[a-z]+$\")\n","\n","def normalize_text(text: str) -> str:\n","    \"\"\"\n","    - Unicode NFKC (fixes curly quotes/spacing)\n","    - Convert curly quotes to straight quotes\n","    - Lowercase\n","    - Collapse multiple spaces\n","    \"\"\"\n","    if not isinstance(text, str):\n","        return text\n","    text = unicodedata.normalize(\"NFKC\", text)\n","    text = (text.replace(\"\\u2019\", \"'\")\n","                .replace(\"\\u2018\", \"'\")\n","                .replace(\"\\u201C\", '\"')\n","                .replace(\"\\u201D\", '\"'))\n","    text = text.lower()\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","\n","def tokenize(text: str):\n","    \"\"\"Word tokenization after normalization.\"\"\"\n","    if not isinstance(text, str):\n","        return []\n","    return word_tokenize(text)\n","\n","def _fix_contractions(tokens):\n","    \"\"\"\n","    Turn ['aux','n't'] -> ['not'] for auxiliaries and special stems (ca/wo/sha).\n","    Drop possessive \"'s\"/\"’s\".\n","    \"\"\"\n","    fixed = []\n","    i = 0\n","    while i < len(tokens):\n","        t = tokens[i]\n","        # \"aux n't\" -> \"not\" (handle standard auxiliaries and stems)\n","        if i+1 < len(tokens) and tokens[i+1] in {\"n't\", \"n’t\"} and (t in _AUX_FOR_NT or t in _AUX_NT_STEMS):\n","            fixed.append(\"not\")\n","            i += 2\n","            continue\n","        # drop possessive clitic\n","        if t in {\"'s\", \"’s\"}:\n","            i += 1\n","            continue\n","        fixed.append(t)\n","        i += 1\n","    return fixed\n","\n","# ---------------- Stopword setup ---------------- #\n","GIST_URL = \"https://gist.githubusercontent.com/rishg2/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\"\n","\n","def load_stopwords():\n","    \"\"\"Merge NLTK and gist stopwords; KEEP negations/intensifiers.\"\"\"\n","    base = set(w.strip().lower() for w in stopwords.words('english'))\n","    try:\n","        resp = requests.get(GIST_URL, timeout=15)\n","        resp.raise_for_status()\n","        extra = {line.strip().lower() for line in resp.text.splitlines() if line.strip()}\n","        merged = base | extra\n","    except Exception:\n","        merged = base\n","    return merged - NEGATIONS - INTENSIFIERS\n","\n","stop_words = load_stopwords()\n","\n","# ---------------- Stopword removal ---------------- #\n","output_dir = '/content/drive/MyDrive/digphil/dialogues_filtered'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","def remove_stopwords(text):\n","    \"\"\"\n","    Normalize → tokenize → fix contractions → drop stopwords →\n","    strip ALL punctuation/numbers (keep only a–z) → join.\n","    \"\"\"\n","    if not isinstance(text, str):\n","        return text\n","\n","    norm = normalize_text(text)\n","    words = tokenize(norm)\n","    words = _fix_contractions(words)\n","\n","    # keep negations/intensifiers even if in stoplist; drop others in stoplist\n","    kept = [w for w in words if (w.lower() not in stop_words) or (w.lower() in NEGATIONS) or (w.lower() in INTENSIFIERS)]\n","\n","    # strip ALL punctuation/numbers: keep only alphabetic tokens\n","    kept = [w for w in kept if _ALPHA_RE.fullmatch(w)]\n","\n","    line = \" \".join(kept).strip()\n","    return line\n","\n","# ---------------- Process all CSVs (CSV ONLY) ---------------- #\n","for csv_path in glob.glob('/content/drive/MyDrive/digphil/dialogues/chapter_*_dialogues.csv'):\n","    df = pd.read_csv(csv_path)\n","\n","    # Apply to every column (safe even if some columns are non-text)\n","    for col in df.columns:\n","        df[col] = df[col].apply(remove_stopwords)\n","\n","    # Optional: drop rows that became entirely empty across all columns\n","    df = df.replace('', np.nan).dropna(how='all')\n","\n","    out_path = os.path.join(output_dir, os.path.basename(csv_path))\n","    df.to_csv(out_path, index=False)\n","    print(f\"Filtered file saved: {out_path}\")\n","\n","print(\"✅ All dialogue files processed and saved to 'dialogues_filtered' folder (CSV only).\")\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLsT07gMnHPqrX9CyJ7Gxl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}