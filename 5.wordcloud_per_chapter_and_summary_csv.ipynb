{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40558,
     "status": "ok",
     "timestamp": 1760014497896,
     "user": {
      "displayName": "Lilin Qiu",
      "userId": "05133583203755337087"
     },
     "user_tz": -120
    },
    "id": "iNDQp7SJ7SJX",
    "outputId": "d09a1095-8e0d-4e6f-ee44-e6e7d3d730e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "‚úÖ Working in: /content/drive/MyDrive/digphil\n",
      "‚úÖ  Saved word cloud for Chapter 1 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_1.png\n",
      "‚úÖ  Saved word cloud for Chapter 2 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_2.png\n",
      "‚úÖ  Saved word cloud for Chapter 3 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_3.png\n",
      "‚úÖ  Saved word cloud for Chapter 4 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_4.png\n",
      "‚úÖ  Saved word cloud for Chapter 5 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_5.png\n",
      "‚úÖ  Saved word cloud for Chapter 6 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_6.png\n",
      "‚úÖ  Saved word cloud for Chapter 7 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_7.png\n",
      "‚úÖ  Saved word cloud for Chapter 8 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_8.png\n",
      "‚úÖ  Saved word cloud for Chapter 9 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_9.png\n",
      "‚úÖ  Saved word cloud for Chapter 10 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_10.png\n",
      "‚úÖ  Saved word cloud for Chapter 11 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_11.png\n",
      "‚úÖ  Saved word cloud for Chapter 12 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_12.png\n",
      "‚úÖ  Saved word cloud for Chapter 13 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_13.png\n",
      "‚úÖ  Saved word cloud for Chapter 14 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_14.png\n",
      "‚úÖ  Saved word cloud for Chapter 15 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_15.png\n",
      "‚úÖ  Saved word cloud for Chapter 16 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_16.png\n",
      "‚úÖ  Saved word cloud for Chapter 17 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_17.png\n",
      "‚úÖ  Saved word cloud for Chapter 18 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_18.png\n",
      "‚úÖ  Saved word cloud for Chapter 19 ‚Üí /content/drive/MyDrive/digphil/wordclouds/chapter_19.png\n",
      "\n",
      "üé®  All word clouds saved in /MyDrive/digphil/wordclouds/\n",
      "üßæ  Summary saved to: /content/drive/MyDrive/digphil/chapter_word_summary.csv\n",
      "üìä  Detailed frequencies saved to: /content/drive/MyDrive/digphil/chapter_word_frequencies_top50.csv\n"
     ]
    }
   ],
   "source": [
    "# === 0) Mount Google Drive ===\n",
    "from google.colab import drive\n",
    "try:\n",
    "    drive.flush_and_unmount()\n",
    "except Exception:\n",
    "    pass\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# === 1) Set project working directory on Drive ===\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/digphil')\n",
    "print(\"‚úÖ Working in:\", os.getcwd())\n",
    "\n",
    "# === 2) Imports ===\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# === 3) NLTK data ===\n",
    "nltk.download('stopwords', quiet=True)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    try:\n",
    "        nltk.download('punkt_tab', quiet=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# === 4) Stopwords ===\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# === 5) Folders on Drive ===\n",
    "dialogue_dir = 'dialogues_filtered'                         # input folder on Drive\n",
    "output_dir = '/content/drive/MyDrive/digphil/wordclouds'    # output folder on Drive\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === 6) Helper: safe tokenizer ===\n",
    "def safe_tokenize(text: str):\n",
    "    \"\"\"Try NLTK word_tokenize; fall back to wordpunct_tokenize if resources missing.\"\"\"\n",
    "    try:\n",
    "        return word_tokenize(text)\n",
    "    except LookupError:\n",
    "        return wordpunct_tokenize(text)\n",
    "\n",
    "# === 7) Collect outputs for CSVs ===\n",
    "summary_rows = []             # compact per-chapter list of top words\n",
    "long_rows = []                # long-form: chapter, word, frequency (top 50)\n",
    "\n",
    "# === 8) Generate word clouds and CSV details for chapters 1‚Äì19 ===\n",
    "for i in range(1, 20):\n",
    "    filename = f'chapter_{i}_dialogues.csv'\n",
    "    filepath = os.path.join(dialogue_dir, filename)\n",
    "\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(f\"‚ö†Ô∏è  Warning: {filename} not found.\")\n",
    "        continue\n",
    "\n",
    "    # Read and combine text\n",
    "    # If your CSV has a 'dialogue' column, this will use it; otherwise fall back to first column\n",
    "    df = pd.read_csv(filepath, encoding='utf-8', on_bad_lines='skip')\n",
    "    if 'dialogue' in df.columns:\n",
    "        text_series = df['dialogue'].astype(str)\n",
    "    else:\n",
    "        # fall back to first column\n",
    "        text_series = df.iloc[:, 0].astype(str)\n",
    "\n",
    "    text = ' '.join(text_series.tolist())\n",
    "\n",
    "    # Tokenize & clean (lowercase, keep a‚Äìz only, remove stopwords)\n",
    "    tokens = safe_tokenize(text.lower())\n",
    "    words = [w for w in tokens if re.fullmatch(r'[a-z]+', w) and w not in stop_words]\n",
    "\n",
    "    # Count top 50 words\n",
    "    top50 = Counter(words).most_common(50)\n",
    "    if not top50:\n",
    "        print(f\"‚ö†Ô∏è  No content for Chapter {i}.\")\n",
    "        continue\n",
    "\n",
    "    # Build frequency dict for word cloud\n",
    "    freq_dict = dict(top50)\n",
    "\n",
    "    # Generate word cloud (high-res)\n",
    "    wc = WordCloud(width=1600, height=800, background_color='white', colormap='viridis')\n",
    "    wc.generate_from_frequencies(freq_dict)\n",
    "\n",
    "    # Save image\n",
    "    out_img = os.path.join(output_dir, f'chapter_{i}.png')\n",
    "    wc.to_file(out_img)\n",
    "    print(f\"‚úÖ  Saved word cloud for Chapter {i} ‚Üí {out_img}\")\n",
    "\n",
    "    # Add compact summary row (top words only, comma-separated)\n",
    "    summary_rows.append({\n",
    "        'chapter': f'chapter_{i}',\n",
    "        'most_frequent_words': ', '.join([w for w, c in top50])\n",
    "    })\n",
    "\n",
    "    # Add long-form rows (word + frequency)\n",
    "    for w, c in top50:\n",
    "        long_rows.append({'chapter': i, 'word': w, 'frequency': c})\n",
    "\n",
    "# === 9) Save CSVs to Drive ===\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_csv_path = '/content/drive/MyDrive/digphil/chapter_word_summary.csv'\n",
    "summary_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "long_df = pd.DataFrame(long_rows).sort_values(['chapter', 'frequency'], ascending=[True, False])\n",
    "long_csv_path = '/content/drive/MyDrive/digphil/chapter_word_frequencies_top50.csv'\n",
    "long_df.to_csv(long_csv_path, index=False)\n",
    "\n",
    "print(\"\\nüé®  All word clouds saved in /MyDrive/digphil/wordclouds/\")\n",
    "print(f\"üßæ  Summary saved to: {summary_csv_path}\")\n",
    "print(f\"üìä  Detailed frequencies saved to: {long_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNXX4KCeKqDv+BrRRbeX3Lc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
